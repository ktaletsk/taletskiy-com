---
title: Какие локальные модели реально работают с Claude Code на MacBook Pro с 48 ГБ?
date: 2026-01-15
draft: false
summary: "Небольшой эксперимент по оценке локальных моделей для агентных задач в Claude Code"
tags: ["Claude Code", "Ollama", "Локальные модели", "qwen", "devstral"]
image: /img/ollama-claude-code-hero.png
---

![Claude Code работает с devstral-128k через Ollama](/img/ollama-claude-code-hero.png)

## Я протестировал 18 локальных моделей, чтобы вам не пришлось

Ollama [выпустила](https://github.com/ollama/ollama/releases/tag/v0.14.0) совместимость с Anthropic API в январе 2026 года, поэтому я протестировал **18 локальных моделей** с Claude Code, чтобы выяснить, какие из них реально работают для агентных задач программирования.

> **Коротко**
> 1. [`devstral-small-2:24b`](https://ollama.com/library/devstral-small-2) — победитель: лучшее качество, самая быстрая, ноль вмешательств
> 2. **ОБЯЗАТЕЛЬНО настройте окно контекста** — Ollama по умолчанию использует 4K; используйте минимум 64K
> 3. **Ожидайте 12-24 минуты на задачи, которые занимают ~2 минуты с Opus 4.5** — но это работает!

- Документация Ollama: https://docs.ollama.com/integrations/claude-code
- Совместимость с Anthropic API: https://docs.ollama.com/api/anthropic-compatibility

## Моя конфигурация

| Характеристика | Значение |
| -------------- | -------------------- |
| Машина | MacBook Pro |
| Чип | Apple M4 Pro |
| RAM | 48 ГБ unified memory |
| Ollama | v0.14.2 |

## Модели

Вот всё, что я протестировал, отсортировано по размеру:

| Модель | Размер | Релиз | SWE-bench | Тип |
|--------|--------|-------|-----------|-----|
| nemotron-3-nano:30b | 24GB | Дек 2025 | - | MoE |
| cogito:32b | 20GB | Июл 2025 | - | Гибридное рассуждение |
| granite4:32b-a9b-h | ~20GB | Окт 2025 | - | Универсальная |
| command-r:35b | 19GB | Мар 2024 | - | RAG-оптимизированная |
| qwen2.5-coder:32b | 19GB | Ноя 2024 | 9.0% | Программирование |
| deepseek-r1:32b | 19GB | Янв 2025 | 41.4% | Рассуждение |
| qwen3-coder:30b | 18GB | Июл 2025 | 51.6% | Программирование |
| qwen3:30b | 18GB | Апр 2025 | - | Универсальная |
| devstral-small-2:24b | 15GB | Дек 2025 | 68.0% | Агентное программирование |
| mistral-small3.2:24b | 15GB | Июн 2025 | - | Универсальная |
| magistral:24b | 14GB | Июн 2025 | - | Рассуждение |
| gpt-oss:20b | 14GB | Авг 2025 | - | Универсальная |
| cogito:14b | 9GB | Июл 2025 | - | Гибридное рассуждение |
| deepseek-coder-v2:16b | 8.9GB | Июн 2024 | - | Программирование (без инструментов) |
| rnj-1:8b | 5.1GB | Дек 2025 | 20.8% | Универсальная |
| phi4-mini:3.8b | 2.5GB | Фев 2025 | - | Универсальная |
| granite4:3b | 2.1GB | Окт 2025 | - | Универсальная |
| functiongemma:270m | 301MB | Дек 2025 | - | Вызов функций |

## Эксперименты

Я выбрал очень простую задачу: запустить `/init` на репозитории (`jupyterlab-latex`) для генерации CLAUDE.md — обычно это первое, что я делаю в новом репозитории. Однако это обманчиво сложно — модель должна обнаружить инструменты, исследовать несколько файлов и синтезировать документацию без галлюцинаций. Один-два запуска на модель; результаты следует рассматривать как полевые заметки.

Мои первые две модели (nemotron, gpt-oss) использовали окно контекста Ollama по умолчанию — так я обнаружил проблему с лимитом в 4K. После этого я установил контекст 64K+ в настройках Ollama.

### `nemotron-3-nano:30b`

Моя первая попытка выявила критический режим отказа. С окном контекста по умолчанию блок размышлений модели явно показывает, что она решила полностью пропустить чтение файлов:

> *«У нас нет деталей репозитория... Пока не было никаких чтений... Предположим типичную структуру репозитория»*

Вместо использования инструментов для исследования, она **выдумала целую структуру кодовой базы**. Вывод описывал монорепозиторий React/Node.js с директориями `/frontend` и `/backend` — ни одной из которых не существует в jupyterlab-latex (Python/TypeScript расширение JupyterLab). Она придумала команды вроде `npm run dev` и ссылалась на несуществующие конфигурационные файлы.

Этот сбой привёл меня к обнаружению лимита контекста Ollama по умолчанию в 4K. После настройки окна контекста в 128K последующие попытки работали намного лучше:
```
Read → Glob → Read → Read → Read → Read → Glob → Read → Write
```

Модель правильно исследовала кодовую базу, но всё же остановилась в середине задачи и потребовала дополнительной подсказки («Continue»), чтобы закончить. Финальный вывод был точным и высокого качества — доказывая, что модель *может* работать, но настройка контекста критически важна.

### `gpt-oss:20b`

Также тестировалась в начале с окном контекста по умолчанию. Быстрая, но ненадёжная:
- Прямой промпт: Быстро завершила, но низкое качество вывода
- Скилл `/init`: Ошибки параметров инструментов, пустые результаты, требовалось вмешательство

```
Готовилось 2м 37с  (таймер задач Claude Code)
```

### `devstral-small-2:24b` ⭐ Победитель

С контекстом 128K, настроенным с самого начала, это был **идеальный запуск**. Модель сразу поняла задачу:

> «Я проанализирую эту кодовую базу и создам файл CLAUDE.md с важной информацией для будущих инстансов.»

Последовательность вызовов инструментов показывает прямое, уверенное использование:
```
Bash → Bash → Bash → Read → Bash → Bash → Bash → Read → Read → Read → Bash → Write
```

Никакой путаницы с субагентами или параметрами инструментов — она сразу использовала `Bash` и `Read` для исследования кодовой базы, затем `Write` для создания вывода.

Вывод составил 180 строк документации с реальными именами функций, примерами конфигурации Python и диаграммой потока коммуникации из 5 шагов. Каждая ссылка на файл подтвердилась — никаких галлюцинаций.

Почему devstral превзошла остальных? Mistral тренировала её специально для SWE-Bench (68.0% результат) и сценариев использования инструментов. Это видно в вызовах инструментов — прямые и уверенные, никакой путаницы с субагентами.

```
Готовилось 17м 12с
```

### `qwen3-coder:30b`

Также настроена с контекстом 128K. Первым инстинктом модели было делегировать субагенту. Из трассировки сессии видно, что она дважды пыталась создать Explore-агента:

```json
{
  "description": "Explore codebase structure",
  "prompt": "Explore the structure of this JupyterLab LaTeX extension repository...",
  "subagent_type": "Explore"
}
```

Это не баг Ollama, а несоответствие между **тем, что Claude Code может делать в данном окружении** и **тем, что модель решает попробовать**. Claude Code имеет концепцию субагентов (вроде помощника «Explore»), но в моей конфигурации они не были доступны/настроены, поэтому этот вызов инструмента не срабатывает. Документация Ollama рекламирует использование Claude Code, так что стоит явно отметить: со сторонними моделями следует ожидать периодических «странностей с инструментами» подобного рода, даже если транспортный API совместим.

Когда инструмент Task не сработал (субагенты не были настроены), qwen3-coder изящно адаптировалась. Последовательность инструментов показывает восстановление:
```
Task → Task → Bash → Read → Read → Read → Read → Read → Read → Read → Read → Write
```

После двух неудачных попыток Explore она переключилась на прямые инструменты `Bash` и `Read` и завершила задачу без дальнейшего вмешательства. Качество вывода было хорошим — точно, без галлюцинаций, но менее детально, чем у devstral (86 строк против 180).

```
Готовилось 23м 48с
```

### `granite4:32b-a9b-h`

Интересная точка сравнения — это универсальная модель IBM на 32B, не специалист по программированию. С настроенным контекстом 128K она завершила задачу **менее чем за 7 минут** — самый быстрый успешный запуск.

Компромисс: минимальное исследование. Последовательность инструментов:
```
Read → Write
```

Всего два вызова инструментов — прочитала README, записала CLAUDE.md. Никакого исследования кодовой базы, никакой проверки package.json, никакого анализа архитектуры. Вывод был приемлемым:
- ✅ Правильный тип проекта (JupyterLab LaTeX расширение)
- ✅ Правильные команды (`jlpm run build`, `jlpm run watch`)
- ✅ Mermaid-диаграмма архитектуры
- ⚠️ Некоторые галлюцинированные детали (ссылалась на `src/components/Toolbar.tsx` без проверки его существования)

При 32K контексте она зависла — начала правильно (Glob → Read), но застряла после чтения файлов и никогда не выдала результат. Другой режим отказа, чем галлюцинации devstral при 32K.

**Вердикт:** Работает, но ленивая. Универсальные модели могут выполнять агентные задачи, но склонны «импровизировать» с минимальным использованием инструментов, в то время как специалисты по программированию исследуют более тщательно.

```
Готовилось ~7м
```

### `qwen3:30b`

Универсальная Qwen3 (не coder-вариант). Это был худший результат — **чистые галлюцинации с нулевым исследованием**.

Последовательность инструментов:
```
Write
```

Всего один вызов инструмента. Блок размышлений показателен — она явно признала, что не может видеть файлы, но всё равно продолжила:

> *«Поскольку я не могу реально видеть файлы, мне придётся полагаться на предоставленный контекст.»*

Она вывела структуру файлов из git status в системном промпте, затем выдумала всё остальное:
- ❌ `python jupyterlab_latex/build.py` — неправильная команда (должна быть `jlpm run build`)
- ❌ `latex_cleanup.py` — выдуманное имя файла
- ❌ `flake8` — предположила линтер без проверки

При контексте 128K она потребляла **31 ГБ RAM** (против 18 ГБ на диске) — нагружая мою систему с 48 ГБ до свопа. Нагрузка на память могла способствовать её лени, но блок размышлений показывает, что она сознательно выбрала угадывание вместо исследования.

**Ключевой вывод:** Файн-тюнинг для программирования — это не только знания о коде, он учит модель реально использовать инструменты вместо угадывания. qwen3-coder исследовала правильно; базовая qwen3 всё галлюцинировала.

```
Готовилось ~5м
```

### `qwen2.5-coder:32b`

**Провал.** Несмотря на настроенный контекст 128K, через несколько попыток она продолжала тянуться к инструменту субагента `Explore`, а затем резко останавливалась, не завершая никакой работы. В отличие от qwen3-coder, которая восстановилась при неудаче Explore, qwen2.5-coder не смогла адаптироваться. Та же семья моделей, разное поколение, совершенно разное поведение при сбоях.

### `mistral-small3.2:24b`

**Провал — галлюцинировала параметры инструментов.** Эта модель понимает, что должна использовать инструменты, но изобретает неправильные схемы параметров. Из трассировки сессии видно, что она пыталась вызвать инструмент Task с выдуманными параметрами:

```json
// Попытка 1:
{"instruction": "...", "max_depth": 100}

// Попытка 2:
{"subagent_name": "Explore", "subagent_type": "Explore", "subagent_prompt": "..."}
```

Реально требуемые параметры — `description` и `prompt`. Когда она получила чёткие сообщения об ошибках, объясняющие это, она просто повторила «Я собираюсь использовать инструмент Task...» и остановилась — неспособная к самокоррекции.

Это другой режим отказа, чем галлюцинирование контента (qwen3) или отказ (functiongemma). Модель узнала *об* инструментах, но не о реальном формате вызова. Стоит отметить: devstral-small-2 тоже модель Mistral и работает отлично — разница в агентной специализации devstral.

**Память:** 37 ГБ загружено при контексте 128K (против 15 ГБ на диске).

### `magistral:24b`

**Провал — описывала инструменты вместо их вызова.** Эта новая reasoning-модель Mistral поняла задачу и знала, какие инструменты использовать, но записывала вызовы инструментов как текст вместо их реального выполнения:

~~~
«Позвольте мне использовать инструмент Glob для поиска этих паттернов:

```bash
Glob pattern: **/README.md
Glob pattern: .github/readme*
...
```

Теперь, когда у меня есть релевантные файлы, давайте проанализируем...»
~~~

Ноль реальных вызовов инструментов. Модель описывала, что бы она *сделала*, предполагала, что инструменты запустились, и переходила к следующему шагу. Это предполагает обучение на документации инструментов без реальных взаимодействий с инструментами.

**Память:** 23 ГБ загружено при контексте 128K (против 14 ГБ на диске).

**Ограничение нативного контекста:** нативный контекст magistral только 39K. Даже с выделенными Ollama 128K модель может неэффективно использовать контекст за пределами своего тренировочного лимита — что может объяснить, почему она никогда не получила формат вызова инструментов.

### `cogito:32b`

**Провал — проблемы с памятью и застревание на контексте.** Эта гибридная reasoning-модель имеет разные режимы отказа в зависимости от настройки контекста:

**При контексте 128K:** Загрузила 64 ГБ в память (41% CPU / 59% GPU распределение). На моей системе с 48 ГБ это вызвало серьёзные проблемы с памятью — скачки нагрузки на память, использование свопа и ноль сгенерированных токенов после 5+ минут.

**При контексте 64K:** Загрузила 42 ГБ (8% CPU / 92% GPU). Всё ещё тесно, но работает. То же поведение застревания.

**При контексте 32K:** Загрузила 30 ГБ (100% GPU). Реально начала работать! Сделала правильные вызовы Glob и Read, исследовала кодовую базу правильно:

```
Glob → Read README.md → «Позвольте мне создать список задач...»
```

Но затем она просто... остановилась. Сказала «Позвольте мне начать с написания раздела обзора сначала» и завершилась без написания чего-либо. Даже подталкивание промптом «continue» не помогло — полностью застряла.

Это тот же паттерн, что у granite4:32b при контексте 32K: может исследовать, но не может завершить. **32K контекста недостаточно для завершения задачи** — модель теряет цель в процессе выполнения.

### `cogito:14b`

**Провал — множественные проблемы с инструментами.** Тестирование меньшего варианта cogito, чтобы увидеть, есть ли сюрпризы в диапазоне 7-15B. Были, но не хорошие.

**Память:** Даже при 9 ГБ на диске загрузила 45 ГБ при контексте 128K с 15% оффлоадом на CPU. При контексте 64K было более управляемо.

Последовательность инструментов показывает несколько режимов отказа:
```
Read README.md ✅ → Read copilot-instructions.md ✅ (не найден) →
WebSearch ❌ (галлюцинация) → TodoWrite ❌ (неправильные параметры, дважды) →
Напечатала CLAUDE.md как текст ⚠️
```

1. **Галлюцинировала `WebSearch`** — инструмент не существует в Claude Code, получила пустые результаты
2. **Неправильные параметры TodoWrite** — отсутствует обязательное поле `activeForm`, пробовала дважды без обучения
3. **Никогда не использовала инструмент Write** — просто напечатала содержимое CLAUDE.md как markdown-текст вместо записи в файл

Сгенерированный контент был на самом деле разумным — правильные команды, точная архитектура. Но модель «завершила» задачу, напечатав вывод, а не записав файл. Она поняла цель, но не смогла правильно выполнить.

**Время:** ~7.7 минут

Семья cogito (как 32b, так и 14b) постоянно терпит неудачу со схемами инструментов Claude Code — разные размеры, разные режимы отказа, тот же результат.

### `command-r:35b`

**Провал — вложенная схема параметров инструментов.** Последняя непротестированная модель в жизнеспособном диапазоне 15-35B. При контексте 128K она не поместилась на моём GPU. При 64K и 32K она загрузилась, но провалилась с той же проблемой схемы инструментов.

Из трассировки видно, что модель обернула все параметры инструментов во вложенную структуру:

```json
{
  "tool_name": "Task",
  "parameters": {
    "description": "...",
    "prompt": "...",
    "subagent_type": "general-purpose"
  }
}
```

Правильный формат — плоские параметры на верхнем уровне. Она сделала 4 вызова инструментов (3 Task, 1 TodoWrite) — все провалились с ошибками валидации вроде «required parameter `description` is missing», потому что вложенность привела к тому, что параметры стали undefined на ожидаемом уровне.

В отличие от mistral-small3.2, которая изобретала неправильные *имена* параметров, command-r использует правильные имена параметров, но оборачивает их неправильно. Когда она получила ошибки валидации, она не повторила попытку — просто выдала текстовый «Action Plan» и остановилась.

Это предполагает, что формат вызова инструментов Cohere отличается от схемы Anthropic API. Модель была обучена на другой структуре вызова инструментов.

**Сравнение контекста:**
- **32K**: 4 вызова инструментов, все провалились, быстро сдалась (~7 мин)
- **64K**: 29 вызовов инструментов, все провалились, продолжала повторять ту же сломанную схему (~9.5 мин)

Больший контекст не помог — он просто дал модели больше времени продолжать проваливаться тем же образом. Она никогда не научилась на сообщениях об ошибках.

## Результаты

### ✅ Работает

| Модель | Качество | Время | Заметки |
|--------|----------|-------|---------|
| **devstral-small-2** ⭐ | Отлично | 17 мин | Без галлюцинаций, без вмешательств |
| qwen3-coder | Хорошо | 24 мин | Восстановилась после неудачи Explore |
| granite4:32b | Хорошо | ~7 мин | Быстрая, но ленивая, мелкие галлюцинации* |

### ⚠️ Завершила с проблемами

| Модель | Качество | Время | Проблема |
|--------|----------|-------|----------|
| gpt-oss:20b | Низкое | ~3 мин | Требовалось вмешательство |
| nemotron-3-nano | Смешанное | - | Галлюцинировала с первой попытки |
| qwen3:30b | Плохое | ~5 мин | Ноль вызовов инструментов, выдумала всё |

### ❌ Провал

| Модель | Время | Режим отказа |
|--------|-------|--------------|
| qwen2.5-coder:32b | - | Застряла на субагенте Explore |
| mistral-small3.2:24b | - | Неправильная схема параметров |
| magistral:24b | - | Описывала инструменты вместо вызова |
| cogito:32b | - | Проблемы с памятью, застревание на контексте |
| cogito:14b | ~8 мин | Галлюцинировала инструмент WebSearch |
| command-r:35b | 7-10 мин | Вложенные параметры инструментов |
| deepseek-r1:32b | - | Нет поддержки инструментов в Ollama |
| deepseek-coder-v2:16b | - | Нет поддержки инструментов в Ollama |
| functiongemma:270m | - | Отказывается от всего |
| granite4:3b | - | Галлюцинирует без инструментов |
| phi4-mini:3.8b | - | Изобретает фейковые имена инструментов |
| rnj-1:8b | - | Тишина, ноль вывода |

*granite4:32b ссылалась на файлы, существование которых никогда не проверяла. Она «работает» в смысле, что завершает задачу и выдаёт пригодный вывод, но его стоит проверить перед доверием. devstral и qwen3-coder надёжны из коробки.

**Победитель: devstral-small-2** — лучшее качество, наименьший footprint, ноль вмешательств.

### Выходы моделей

Сравните реальные файлы CLAUDE.md, сгенерированные каждой моделью. Используйте вкладки для переключения между моделями или нажмите кнопку side-by-side для их прямого сравнения:

{{< text-compare files="devstral-small-2,qwen3-coder,granite4-32b,nemotron-30b" height="600px" >}}

### Режимы отказа

Тестирование выявило различные способы, которыми модели терпят неудачу в агентных задачах:

| Режим отказа | Пример | Вероятная причина |
|--------------|--------|-------------------|
| **Отказывается** | functiongemma | Слишком консервативная, путается в системных промптах |
| **Галлюцинирует контент** | qwen3:30b, granite4:3b | Пропускает инструменты, выдумывает вывод |
| **Галлюцинирует инструменты** | phi4-mini | Изобретает несуществующие имена инструментов |
| **Галлюцинирует параметры** | mistral-small3.2 | Знает, что инструменты существуют, неправильная схема |
| **Описывает инструменты** | magistral | Описывает инструменты текстом, никогда не вызывает |
| **Застревает на субагенте** | qwen2.5-coder | Не может адаптироваться при неудаче Explore |
| **Застревание на контексте** | cogito:32b, granite4@32K | Исследует правильно, останавливается в середине задачи |
| **Вложенные параметры** | command-r | Оборачивает параметры в {"tool_name":X,"parameters":{...}} |
| **Тишина** | rnj-1:8b | Ноль вывода, не может обработать системные промпты |

Более сложные отказы (неправильные параметры, описание, вложенные параметры) предполагают модели, обученные на разных форматах вызова инструментов или документации, а не на реальных взаимодействиях с Anthropic API. Нативное окно контекста тоже важно — magistral (39K нативно) провалилась даже с выделенными 128K.

### Как локальные модели сравниваются с облаком

SWE-bench Verified — это то, что все используют для оценки агентного программирования — 500 реальных GitHub issues, которые модели должны решить. Вот как локальные модели сравниваются с облаком:

**Frontier облачные модели (Проприетарные)**

| Модель | SWE-bench |
|--------|-----------|
| Gemini 3 Flash | 75-76% |
| Claude Opus 4.5 | 74-81% |
| GPT-5.2 | 72-75% |
| Claude Sonnet 4.5 | 70.6% |
| Claude Haiku 4.5 | 68.8% |

**Большие модели с открытыми весами (Не помещаются в 48 ГБ)**

| Модель | SWE-bench | Размер |
|--------|-----------|--------|
| Devstral 2 | 72.2% | 123B |
| Qwen3-Coder-480B | 67% | 480B |
| DeepSeek-V3.1 | 66% | 671B |

**Локальные модели (Помещаются в 48 ГБ)**

| Модель | SWE-bench | Результат |
|--------|-----------|-----------|
| **devstral-small-2** | **68.0%** | ⭐ Победитель |
| qwen3-coder:30b | 51.6% | ✅ Хорошо |
| deepseek-r1:32b | 41.4% | ❌ Нет инструментов |
| qwen2.5-coder:32b | 9.0% | ❌ Застряла |

**Разрыв удивительно мал.** devstral-small-2 с 68% соответствует Claude Haiku 4.5 и отстаёт от Opus всего на 6-8 пунктов. Модель на 24B, работающая локально, держится наравне с моделями 100B+ — оказывается, агентное обучение важнее размера.

Результат SWE-bench также предсказывает успех с Claude Code: модели без опубликованных результатов не сфокусированы на программировании и провалили мои тесты.

## Выводы

Локальные модели теперь могут делать реальную агентную работу. devstral-small-2 надёжно завершила задачу без помощи. Она медленнее облака (17 мин против 2 мин), но работает на моём ноутбуке полностью оффлайн.

### Ключевые выводы

1. **devstral-small-2 побеждает** — лучшие результаты, наименьший footprint, создана для этого
2. **Разрыв меньше, чем я ожидал** — 68% SWE-bench соответствует Haiku, отстаёт от Opus на 8 пунктов
3. **Окно контекста важно** — Ollama по умолчанию 4K; увеличьте до 64K или смотрите, как модели галлюцинируют
4. **SWE-bench предсказывает успех** — отсутствие опубликованного результата обычно означает, что работать не будет
5. **Скорость страдает** — 17-24 минуты против 2 минут в облаке
6. **Сначала проверьте поддержку инструментов** — не все модели работают с Anthropic API Ollama

### Что работает

devstral-small-2 и qwen3-coder обе работают надёжно. Инфраструктура вызова инструментов солидная, когда модель это поддерживает. Ollama 0.14.0 делает настройку простой — больше никакого слоя трансляции LiteLLM.

### Что не работает (пока)

Большинство моделей не могут завершить многошаговые агентные задачи без помощи. Переполнение контекста вызывает галлюцинации (выдуманные URL, неправильные имена репозиториев). И в 8-12 раз медленнее облака сложно игнорировать.

### Критически важно: установите контекст 64K+

Ollama по умолчанию использует контекст 4K независимо от того, что рекламируют карточки моделей. Системные промпты Claude Code переполняют это, вызывая тихие отказы или галлюцинации.

![Настройки Ollama, показывающие слайдер длины контекста](/img/ollama-context-setting.png)

| Контекст | Результат |
|----------|-----------|
| 4-16K | ❌ Ноль вызовов инструментов |
| 32K | ⚠️ Начинает нормально, потом галлюцинирует |
| 64K+ | ✅ Работает |

### Быстрый старт

```bash
# 1. Установите Ollama 0.14.0+ и скачайте devstral
ollama pull devstral-small-2

# 2. Установите контекст 64K в настройках Ollama (GUI слайдер)

# 3. Добавьте алиас в ~/.zshrc
alias claude-local='ANTHROPIC_BASE_URL=http://localhost:11434 ANTHROPIC_API_KEY=ollama CLAUDE_CODE_USE_BEDROCK=0 claude --model devstral-small-2'

# 4. Запустите
source ~/.zshrc
claude-local
```
